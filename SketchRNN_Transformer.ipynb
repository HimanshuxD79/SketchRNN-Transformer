{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Demo.ipynb","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11178369,"sourceType":"datasetVersion","datasetId":6977076}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n\n### Requirements","metadata":{"id":"tnDI7Hog5omz"}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport sys\nfrom copy import deepcopy\nimport os","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nidXb-SvzyQN","outputId":"35fb3c26-08c3-4eb9-c035-8eb0f03eb123","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:25:41.814446Z","iopub.execute_input":"2025-03-26T21:25:41.814948Z","iopub.status.idle":"2025-03-26T21:25:41.819217Z","shell.execute_reply.started":"2025-03-26T21:25:41.814923Z","shell.execute_reply":"2025-03-26T21:25:41.818153Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### MODEL ARCHITECTURE","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\ndef positional_encoding(max_seq_len, dm):\n    \n    PE = np.zeros((max_seq_len, dm))\n    for row in range(max_seq_len):\n        for col in range(0, dm, 2):\n            PE[row, col] = np.sin(row / (10000 ** (col / dm)))\n            PE[row, col + 1] = np.cos(row / (10000 ** (col / dm)))\n    return PE\n\n\ndef sdp_attention(Q, K, V, mask=None):\n    QK = tf.matmul(Q, K, transpose_b=True)\n    dk = tf.cast(tf.shape(K)[-1], tf.float32)\n    scaled = QK / tf.math.sqrt(dk)\n    if mask is not None:\n        mask = tf.expand_dims(tf.expand_dims(mask, 0), 1)\n        scaled += mask * -1e9\n    weights = tf.nn.softmax(scaled, axis=-1)\n    output = tf.matmul(weights, V)\n\n    return output, weights\n\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n   \n    def __init__(self, dm, h):\n        super(MultiHeadAttention, self).__init__()\n        self.h = h\n        self.dm = dm\n        self.depth = dm // self.h\n        self.Wq = tf.keras.layers.Dense(dm)\n        self.Wk = tf.keras.layers.Dense(dm)\n        self.Wv = tf.keras.layers.Dense(dm)\n        self.linear = tf.keras.layers.Dense(dm)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.h, self.depth))\n        x = tf.transpose(x, perm=[0, 2, 1, 3])\n        return x\n\n    def call(self, Q, K, V, mask):\n        batch_size = tf.shape(Q)[0]\n\n        Q = self.Wq(Q)\n        K = self.Wk(K)\n        V = self.Wv(V)\n\n        Q = self.split_heads(Q, batch_size)\n        K = self.split_heads(K, batch_size)\n        V = self.split_heads(V, batch_size)\n\n        attention, weights = sdp_attention(Q, K, V, mask)\n\n        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n        attention = tf.reshape(attention, (batch_size, -1, self.dm))\n        output = self.linear(attention)\n\n        return output, weights\n\n\nclass DecoderBlock(tf.keras.layers.Layer):\n   \n    def __init__(self, dm, h, hidden, drop_rate=0.1, name=None):\n        \n        super(DecoderBlock, self).__init__()\n        if name is not None:\n            self._name = name\n        self.mha1 = MultiHeadAttention(dm, h)\n        self.mha2 = MultiHeadAttention(dm, h)\n        self.dense_hidden = tf.keras.layers.Dense(\n            units=hidden,\n            activation='relu'\n        )\n        self.dense_output = tf.keras.layers.Dense(units=dm)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n        self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n\n    def call(self, inputs, look_ahead_mask, training=False):\n       \n        attn_out, _ = self.mha1(inputs, inputs, inputs, look_ahead_mask)\n        attn_out = self.dropout1(attn_out, training=training)\n\n        out = self.layernorm1(inputs + attn_out)\n\n        dense_output = self.dense_hidden(out)\n        dense_output = self.dense_output(dense_output)\n        dense_output = self.dropout2(dense_output, training=training)\n\n        out = self.layernorm2(out + dense_output)\n\n        return out\n\n\nclass Decoder(tf.keras.Model):\n    \n    def __init__(self,\n                 Nb,                # Number of blocks in model base\n                 No,                # Number of blocks in offset branch\n                 Np,                # Number of blocks in pen state branch\n                 dm,                # Model dimensionality\n                 h,                 # Number of heads used in attention\n                 hidden,            # Hidden layer dimenssionality\n                 max_seq_len,\n                 num_classes,       # Maximum sequence length\n                 drop_rate=0.1):    # Drop rate used in dropout layers\n\n        super(Decoder, self).__init__()\n        self.Nb = Nb\n        self.No = No\n        self.Np = Np\n        self.dm = dm\n        self.num_classes = num_classes\n        # Replace the class_embedding Dense layer with:\n        \n        self.class_embedding = tf.keras.layers.Dense(dm, name='class_embedding')\n        self.layernorm_class = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.projection = tf.keras.layers.Dense(dm, name='base_projection')\n        self.positional_encoding = positional_encoding(max_seq_len, dm)\n        self.dropout = tf.keras.layers.Dropout(drop_rate)\n\n        self.base_blocks = [\n            DecoderBlock(dm, h, hidden, drop_rate,\n            name=\"base_block_\" + str(n)) for n in range(Nb)\n        ]\n        self.offset_blocks = [\n            DecoderBlock(dm, h, hidden, drop_rate,\n            name=\"offset_block_\" + str(n)) for n in range(No)\n        ]\n        self.pen_blocks = [\n            DecoderBlock(dm, h, hidden, drop_rate,\n            name=\"pen_block_\" + str(n)) for n in range(Np)\n        ]\n\n        self.offset_dense = tf.keras.layers.Dense(dm, name='offset_dense')\n        self.offset_out = tf.keras.layers.Dense(2, name='offset_out')\n        self.pen_dense = tf.keras.layers.Dense(dm, name='pen_dense')\n        self.pen_out = tf.keras.layers.Dense(3, name='pen_out',\n                                             activation='softmax')\n\n    def call(self,inputs,labels,look_ahead_mask=None,training=False):       \n        \n        class_emb = self.class_embedding(labels)  \n        class_emb = self.layernorm_class(class_emb)\n        class_emb = tf.expand_dims(class_emb, axis=1)\n        #--------------#\n        # if tf.shape(inputs)[1] == 0:  \n        #     x = class_emb  \n        # else:\n        #     x = self.projection(inputs)  \n        #     x = tf.concat([class_emb, x], axis=1) \n        #--------------#     \n        x = self.projection(inputs)\n        x = tf.concat([class_emb, x], axis=1)\n        seq_len = tf.shape(x)[1]\n        x *= tf.math.sqrt(tf.cast(self.dm, 'float32'))\n        x += self.positional_encoding[:seq_len]\n        # pos_encoding = tf.expand_dims(self.positional_encoding[:seq_len], 0)\n        # x += pos_encoding\n        x = self.dropout(x, training=training)\n\n        for block in self.base_blocks:\n            x = block(x, look_ahead_mask, training=training)\n\n        offset = x\n        for block in self.offset_blocks:\n            offset = block(offset, look_ahead_mask, training=training)\n        offset = self.offset_dense(offset)\n        offset = self.offset_out(offset)\n\n        pen = x\n        for block in self.pen_blocks:\n            pen = block(pen, look_ahead_mask, training=training)\n        pen = self.pen_dense(pen)\n        pen = self.pen_out(pen)\n\n        return offset, pen\n","metadata":{"id":"xCatNYia0qcI","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:25:43.897363Z","iopub.execute_input":"2025-03-26T21:25:43.897677Z","iopub.status.idle":"2025-03-26T21:25:43.929420Z","shell.execute_reply.started":"2025-03-26T21:25:43.897654Z","shell.execute_reply":"2025-03-26T21:25:43.928490Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### DATASET CLEANING AND LOADING","metadata":{}},{"cell_type":"code","source":"# def to_big_strokes(stroke, max_len=250):\n#   result = np.zeros((max_len, 5), dtype=float)\n#   l = len(stroke)\n#   assert l <= max_len\n#   result[0:l, 0:2] = stroke[:, 0:2]\n#   result[0:l, 3] = stroke[:, 2]\n#   result[0:l, 2] = 1 - result[0:l, 3]\n#   result[l:, 4] = 1\n#   return result\n\n# def clean(data, max_length=100):\n#     dataset = []\n#     for sample in data:\n#         if len(sample) <= max_length:\n#             sample = to_big_strokes(sample, max_length)\n#             dataset.append(sample)\n#     dataset = np.asarray(dataset)\n#     return dataset\n\n# class Dataset:   \n#     def __init__(self,filepath,batch_size=32,max_length=250):\n#         self.strokes_list = []\n#         self.labels_list = []\n#         samples_per_class=30000 #samples\n#         self.num_classes = len(filepaths)\n#         for label, filepath in enumerate(filepaths):\n#             data = np.load(filepath, encoding='latin1', allow_pickle=True)\n#             train_data = data['train']\n#             if len(train_data) > samples_per_class:\n#                 indices = np.random.choice(len(train_data), samples_per_class, replace=False)\n#                 train_data = train_data[indices]\n#             else:\n#                 print(f\"Warning: {filepath} has only {len(train_data)} samples, less than {samples_per_class}\")\n            \n#             train = clean(train_data, max_length)\n#             self.strokes_list.append(train)\n\n#             one_hot_labels = np.zeros((len(train), self.num_classes), dtype=np.float32)\n#             one_hot_labels[:, label] = 1  # Set the column corresponding to the class to 1\n#             self.labels_list.append(one_hot_labels)\n#             # self.labels_list.append(np.full((len(train),), label, dtype=np.int32))\n#         self.train_strokes = np.concatenate(self.strokes_list, axis=0) \n#         self.train_labels = np.concatenate(self.labels_list, axis=0)\n#         self.train = tf.data.Dataset.from_tensor_slices((self.train_strokes, self.train_labels))\n#         self.train = self.train.shuffle(buffer_size=len(self.train_strokes)).batch(batch_size)\n","metadata":{"id":"b-5QHfuK1Kpp","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:32:30.327888Z","iopub.execute_input":"2025-03-26T21:32:30.328282Z","iopub.status.idle":"2025-03-26T21:32:30.332580Z","shell.execute_reply.started":"2025-03-26T21:32:30.328248Z","shell.execute_reply":"2025-03-26T21:32:30.331555Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\n\ndef to_big_strokes(stroke, max_len=250):\n    # Initialize the result array\n    result = np.zeros((max_len, 5), dtype=float)\n    l = len(stroke)\n    assert l <= max_len\n    \n    if l > 0:  # Ensure there’s at least one stroke to process\n        # Extract x and y coordinates\n        x = stroke[:, 0]\n        y = stroke[:, 1]\n        \n        # Find min and max for normalization\n        min_x, max_x = np.min(x), np.max(x)\n        min_y, max_y = np.min(y), np.max(y)\n        \n        # Normalize x and y to [0, 1], handling edge cases\n        if max_x > min_x:\n            x_norm = (x - min_x) / (max_x - min_x)\n        else:\n            x_norm = np.zeros_like(x)  # Single-point case\n        if max_y > min_y:\n            y_norm = (y - min_y) / (max_y - min_y)\n        else:\n            y_norm = np.zeros_like(y)  # Single-point case\n        \n        # Assign normalized coordinates\n        result[0:l, 0] = x_norm\n        result[0:l, 1] = y_norm\n        # Assign pen states\n        result[0:l, 3] = stroke[:, 2]  # Pen state (0 for drawing, 1 for up)\n        result[0:l, 2] = 1 - result[0:l, 3]  # 1 for drawing, 0 for up\n    \n    # Set padding indicator\n    result[l:, 4] = 1  # 1 for padded steps\n    \n    return result\n\n# Your existing clean function (unchanged)\ndef clean(data, max_length=100):\n    dataset = []\n    for sample in data:\n        if len(sample) <= max_length:\n            sample = to_big_strokes(sample, max_length)\n            dataset.append(sample)\n    dataset = np.asarray(dataset)\n    return dataset\n\n# Your existing Dataset class (unchanged)\nclass Dataset:   \n    def __init__(self, filepaths, batch_size=32, max_length=250):\n        self.strokes_list = []\n        self.labels_list = []\n        samples_per_class = 30000  # samples\n        self.num_classes = len(filepaths)\n        for label, filepath in enumerate(filepaths):\n            data = np.load(filepath, encoding='latin1', allow_pickle=True)\n            train_data = data['train']\n            if len(train_data) > samples_per_class:\n                indices = np.random.choice(len(train_data), samples_per_class, replace=False)\n                train_data = train_data[indices]\n            else:\n                print(f\"Warning: {filepath} has only {len(train_data)} samples, less than {samples_per_class}\")\n            \n            train = clean(train_data, max_length)\n            self.strokes_list.append(train)\n\n            one_hot_labels = np.zeros((len(train), self.num_classes), dtype=np.float32)\n            one_hot_labels[:, label] = 1  # Set the column corresponding to the class to 1\n            self.labels_list.append(one_hot_labels)\n        \n        self.train_strokes = np.concatenate(self.strokes_list, axis=0) \n        self.train_labels = np.concatenate(self.labels_list, axis=0)\n        self.train = tf.data.Dataset.from_tensor_slices((self.train_strokes, self.train_labels))\n        self.train = self.train.shuffle(buffer_size=len(self.train_strokes)).batch(batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:32:36.959616Z","iopub.execute_input":"2025-03-26T21:32:36.959952Z","iopub.status.idle":"2025-03-26T21:32:36.972150Z","shell.execute_reply.started":"2025-03-26T21:32:36.959922Z","shell.execute_reply":"2025-03-26T21:32:36.970961Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### TRAIN FUNCTION","metadata":{}},{"cell_type":"code","source":"def train_model(Nb,No,Np,dm,h,hidden,max_len,batch_size,epochs,filepath,verbose=1,weights=None):\n    data = Dataset(filepath, batch_size=batch_size, max_length=max_len)\n    num_classes = len(filepaths)\n    model = Decoder(Nb, No, Np, dm, h, hidden, max_len,num_classes)\n    inputs = np.random.uniform(size=(1, max_len - 1, 5))\n    dummy_labels = np.zeros((1, num_classes), dtype=np.float32)\n    dummy_labels[0, 0] = 1.0\n    model(tf.convert_to_tensor(inputs, dtype=tf.float32), \n          tf.convert_to_tensor(dummy_labels, dtype=tf.int32), \n          None)\n    # model(inputs, dummy_labels, None)\n    model.summary()\n   \n    offset_weights = []\n    pen_weights = []\n    for weight in model.trainable_weights:\n        if \"base\" in weight.name:\n            offset_weights.append(weight)\n            pen_weights.append(weight)\n        if \"offset\" in weight.name:\n            offset_weights.append(weight)\n        if \"pen\" in weight.name:\n            pen_weights.append(weight)\n\n    offset_loss_func = tf.keras.losses.MeanSquaredError()\n    pen_loss_func = tf.keras.losses.CategoricalCrossentropy()\n\n    pen_train_loss = tf.keras.metrics.Mean(name='pen_train_loss')\n    offset_train_loss = tf.keras.metrics.Mean(name='offset_train_loss')\n    learning_rate = 0.0001\n   \n    # offset_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    # pen_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    optimizer = tf.keras.optimizers.Adam(learning_rate)\n\n    # mask = 1 - tf.linalg.band_part(tf.ones((max_len - 1, max_len - 1)), -1, 0)\n    mask = 1 - tf.linalg.band_part(tf.ones((max_len, max_len)), -1, 0)\n    offset_losses = []\n    pen_losses = []\n    prev_best = 10000\n\n    def train_step(inputs,labels, real):\n    \n        with tf.GradientTape(persistent=True) as tape:\n            offsets, pen_states = model(inputs,labels, mask, training=True)\n            # Change from offsets[:, :-1, :] to offsets[:, 1:, :]\n            offset_loss = offset_loss_func(real[:, :, :2], offsets[:, :-1, :])\n            # Change from pen_states[:, :-1, :] to pen_states[:, 1:, :]\n            pen_loss = pen_loss_func(real[:, :, 2:], pen_states[:, :-1, :])\n            total_loss = offset_loss + pen_loss  # Combine losses\n\n        # grads = tape.gradient(offset_loss, offset_weights)\n        # offset_optimizer.apply_gradients(zip(grads, offset_weights))\n\n        # grads = tape.gradient(pen_loss, pen_weights)\n        # pen_optimizer.apply_gradients(zip(grads, pen_weights))\n        gradients = tape.gradient(total_loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        offset_train_loss(offset_loss)\n        pen_train_loss(pen_loss)\n\n        del tape\n\n    for epoch in range(epochs):\n\n        offset_train_loss.reset_state()\n        pen_train_loss.reset_state()\n\n        for batch, (strokes, labels) in enumerate(data.train):\n            inputs = strokes[:, :-1, :]  \n            real = strokes[:, 1:, :]     \n            train_step(inputs, labels, real)\n            offset_losses.append(offset_train_loss.result())\n            pen_losses.append(pen_train_loss.result())\n\n            if verbose == 2: \n                if batch % 50 == 0:\n                    if batch % 50 == 0:\n                        print(\"Epoch {}, batch {}: Offset Loss: {} Pen Loss {}\"\n                        .format(\n                            epoch + 1,\n                            batch,\n                            offset_train_loss.result(),\n                            pen_train_loss.result()\n                        ))\n\n        if verbose >= 1:  \n            print(\"Epoch {}: Offset Loss: {:.4f} Pen Loss {:.4f}\".format(\n                epoch + 1,\n                offset_train_loss.result(),\n                pen_train_loss.result()\n            ))\n\n        \n        if offset_train_loss.result() < prev_best:\n            model.save_weights('new_model_weight_v1.h5')\n            prev_best = offset_train_loss.result()\n\n    return model, offset_losses, pen_losses\n\n","metadata":{"id":"2mygUDwq5IZY","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:32:40.601246Z","iopub.execute_input":"2025-03-26T21:32:40.601722Z","iopub.status.idle":"2025-03-26T21:32:40.620565Z","shell.execute_reply.started":"2025-03-26T21:32:40.601678Z","shell.execute_reply":"2025-03-26T21:32:40.619085Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### DATASET","metadata":{}},{"cell_type":"code","source":"Dataset_path = r\"/kaggle/input/sketchdata\"\nfilepaths = []\nfor dirname, _, filenames in os.walk(Dataset_path):\n  for filename in filenames:\n    filepaths.append(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:32:44.087728Z","iopub.execute_input":"2025-03-26T21:32:44.088091Z","iopub.status.idle":"2025-03-26T21:32:44.097230Z","shell.execute_reply.started":"2025-03-26T21:32:44.088062Z","shell.execute_reply":"2025-03-26T21:32:44.096041Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"filepaths","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:32:46.292554Z","iopub.execute_input":"2025-03-26T21:32:46.292925Z","iopub.status.idle":"2025-03-26T21:32:46.298393Z","shell.execute_reply.started":"2025-03-26T21:32:46.292896Z","shell.execute_reply":"2025-03-26T21:32:46.297398Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"['/kaggle/input/sketchdata/sketchrnn_moon.full.npz',\n '/kaggle/input/sketchdata/sketchrnn_banana.npz',\n '/kaggle/input/sketchdata/sketchrnn_eraser.full.npz',\n '/kaggle/input/sketchdata/sketchrnn_ladder.full.npz',\n '/kaggle/input/sketchdata/sketchrnn_door.full.npz']"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"data = Dataset(filepaths, batch_size=64, max_length=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:33:15.378789Z","iopub.execute_input":"2025-03-26T21:33:15.379117Z","iopub.status.idle":"2025-03-26T21:33:25.226743Z","shell.execute_reply.started":"2025-03-26T21:33:15.379092Z","shell.execute_reply":"2025-03-26T21:33:25.225998Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:34:02.461248Z","iopub.execute_input":"2025-03-26T21:34:02.461670Z","iopub.status.idle":"2025-03-26T21:34:02.467492Z","shell.execute_reply.started":"2025-03-26T21:34:02.461640Z","shell.execute_reply":"2025-03-26T21:34:02.466498Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<__main__.Dataset at 0x7afd0ce749a0>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nsmall_model, offset_losses, pen_losses = train_model(\n    8,              # Base blocks\n    4,              # Offset blocks\n    4,              # Pen state blocks\n    256,            # Model dimensionality\n    8,              # Heads\n    1024,            # Hidden units\n    100,            # Max sequence length\n    64,             # Batch size\n    5,              # Epochs\n    filepaths,      # File path\n    2,              # Verbosity\n    )# Weights to load if continuing training\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8ITDUUs08L6","outputId":"1c5ac7f3-ab7f-41c0-93b1-3eba41618c6c","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:37:39.610461Z","iopub.execute_input":"2025-03-26T21:37:39.610858Z","iopub.status.idle":"2025-03-26T22:49:41.720955Z","shell.execute_reply.started":"2025-03-26T21:37:39.610818Z","shell.execute_reply":"2025-03-26T22:49:41.719422Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_32', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_33', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_34', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_35', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_36', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_37', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_38', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_39', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_40', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_41', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_42', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_43', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_44', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_45', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_46', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'decoder_block_47', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"decoder_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ class_embedding (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    │           \u001b[38;5;34m1,536\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ layer_normalization_66               │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    │             \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mLayerNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ base_projection (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m256\u001b[0m)                │           \u001b[38;5;34m1,536\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_66 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_32 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_33 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_34 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_35 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_36 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_37 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_38 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_39 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_40 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_41 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_42 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_43 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_44 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_45 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_46 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_47 (\u001b[38;5;33mDecoderBlock\u001b[0m)      │ ?                           │         \u001b[38;5;34m789,760\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ offset_dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │          \u001b[38;5;34m65,792\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ offset_out (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m2\u001b[0m)                 │             \u001b[38;5;34m514\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ pen_dense (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │          \u001b[38;5;34m65,792\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ pen_out (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m3\u001b[0m)                 │             \u001b[38;5;34m771\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ class_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ layer_normalization_66               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ base_projection (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ decoder_block_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │ ?                           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">789,760</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ offset_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ offset_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ pen_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ pen_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">771</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,772,613\u001b[0m (48.72 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,772,613</span> (48.72 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,772,613\u001b[0m (48.72 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,772,613</span> (48.72 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1, batch 0: Offset Loss: 1.929089903831482 Pen Loss 0.9596592783927917\nEpoch 1, batch 50: Offset Loss: 0.5684215426445007 Pen Loss 0.6788382530212402\nEpoch 1, batch 100: Offset Loss: 0.3455033302307129 Pen Loss 0.5288590788841248\nEpoch 1, batch 150: Offset Loss: 0.2591436803340912 Pen Loss 0.4247969090938568\nEpoch 1, batch 200: Offset Loss: 0.2097185105085373 Pen Loss 0.3595448136329651\nEpoch 1, batch 250: Offset Loss: 0.17787130177021027 Pen Loss 0.31955865025520325\nEpoch 1, batch 300: Offset Loss: 0.15541891753673553 Pen Loss 0.2923281490802765\nEpoch 1, batch 350: Offset Loss: 0.13865190744400024 Pen Loss 0.2722129225730896\nEpoch 1, batch 400: Offset Loss: 0.12566378712654114 Pen Loss 0.25726014375686646\nEpoch 1, batch 450: Offset Loss: 0.11524899303913116 Pen Loss 0.2457573413848877\nEpoch 1, batch 500: Offset Loss: 0.10672842711210251 Pen Loss 0.23612147569656372\nEpoch 1, batch 550: Offset Loss: 0.0996202602982521 Pen Loss 0.22865808010101318\nEpoch 1, batch 600: Offset Loss: 0.09356863796710968 Pen Loss 0.222162127494812\nEpoch 1, batch 650: Offset Loss: 0.08840098232030869 Pen Loss 0.2167438268661499\nEpoch 1, batch 700: Offset Loss: 0.08392108231782913 Pen Loss 0.21196772158145905\nEpoch 1, batch 750: Offset Loss: 0.07998787611722946 Pen Loss 0.20775233209133148\nEpoch 1, batch 800: Offset Loss: 0.07650670409202576 Pen Loss 0.20411144196987152\nEpoch 1, batch 850: Offset Loss: 0.07341576367616653 Pen Loss 0.20075488090515137\nEpoch 1, batch 900: Offset Loss: 0.07065770775079727 Pen Loss 0.19787216186523438\nEpoch 1, batch 950: Offset Loss: 0.06816013902425766 Pen Loss 0.19515448808670044\nEpoch 1, batch 1000: Offset Loss: 0.06590640544891357 Pen Loss 0.1927618384361267\nEpoch 1, batch 1050: Offset Loss: 0.06385891884565353 Pen Loss 0.19063705205917358\nEpoch 1, batch 1100: Offset Loss: 0.06199953705072403 Pen Loss 0.18875910341739655\nEpoch 1, batch 1150: Offset Loss: 0.06026668846607208 Pen Loss 0.18686644732952118\nEpoch 1, batch 1200: Offset Loss: 0.058696869760751724 Pen Loss 0.18528026342391968\nEpoch 1, batch 1250: Offset Loss: 0.0572323314845562 Pen Loss 0.1837155669927597\nEpoch 1, batch 1300: Offset Loss: 0.05587752163410187 Pen Loss 0.18222086131572723\nEpoch 1, batch 1350: Offset Loss: 0.05460555478930473 Pen Loss 0.18082992732524872\nEpoch 1, batch 1400: Offset Loss: 0.05341517552733421 Pen Loss 0.1795833855867386\nEpoch 1, batch 1450: Offset Loss: 0.0522940568625927 Pen Loss 0.1783830225467682\nEpoch 1, batch 1500: Offset Loss: 0.05125299096107483 Pen Loss 0.1772414743900299\nEpoch 1, batch 1550: Offset Loss: 0.05025643855333328 Pen Loss 0.17617912590503693\nEpoch 1, batch 1600: Offset Loss: 0.049319393932819366 Pen Loss 0.17516715824604034\nEpoch 1, batch 1650: Offset Loss: 0.04843598231673241 Pen Loss 0.17421646416187286\nEpoch 1, batch 1700: Offset Loss: 0.04759679362177849 Pen Loss 0.1732812076807022\nEpoch 1, batch 1750: Offset Loss: 0.04680733010172844 Pen Loss 0.17242273688316345\nEpoch 1, batch 1800: Offset Loss: 0.046053145080804825 Pen Loss 0.17159825563430786\nEpoch 1, batch 1850: Offset Loss: 0.04533443599939346 Pen Loss 0.1707964688539505\nEpoch 1, batch 1900: Offset Loss: 0.04465267062187195 Pen Loss 0.17003370821475983\nEpoch 1, batch 1950: Offset Loss: 0.04400654137134552 Pen Loss 0.16937537491321564\nEpoch 1, batch 2000: Offset Loss: 0.043386299163103104 Pen Loss 0.16869835555553436\nEpoch 1, batch 2050: Offset Loss: 0.04279753938317299 Pen Loss 0.1680307388305664\nEpoch 1, batch 2100: Offset Loss: 0.042224783450365067 Pen Loss 0.16738320887088776\nEpoch 1, batch 2150: Offset Loss: 0.0416770800948143 Pen Loss 0.1667701154947281\nEpoch 1, batch 2200: Offset Loss: 0.04115324467420578 Pen Loss 0.16620151698589325\nEpoch 1, batch 2250: Offset Loss: 0.040650755167007446 Pen Loss 0.16560588777065277\nEpoch 1, batch 2300: Offset Loss: 0.040171362459659576 Pen Loss 0.16505788266658783\nEpoch 1: Offset Loss: 0.0399 Pen Loss 0.1647\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-4656abc63894>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m small_model, offset_losses, pen_losses = train_model(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m              \u001b[0;31m# Base blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m              \u001b[0;31m# Offset blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-703131f90125>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(Nb, No, Np, dm, h, hidden, max_len, batch_size, epochs, filepath, verbose, weights)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprev_best\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new_model_weight_v1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mprev_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffset_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(model, filepath, overwrite, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;34m\"The filename must end in `.weights.h5`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34mf\"Received: filepath={filepath}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The filename must end in `.weights.h5`. Received: filepath=new_model_weight_v1.h5"],"ename":"ValueError","evalue":"The filename must end in `.weights.h5`. Received: filepath=new_model_weight_v1.h5","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T21:36:59.237043Z","iopub.execute_input":"2025-03-26T21:36:59.237432Z","iopub.status.idle":"2025-03-26T21:36:59.242698Z","shell.execute_reply.started":"2025-03-26T21:36:59.237399Z","shell.execute_reply":"2025-03-26T21:36:59.241671Z"}},"outputs":[{"name":"stdout","text":"Num GPUs Available: 2\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"small_model.load_weights('epoch_best.h5')","metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def predict(model, input_strokes, label):\n    inputs = input_strokes[np.newaxis, :]  # Shape: [1, seq_len, 5]\n    label = label[np.newaxis, :]  # Shape: [1, num_classes]\n    \n    # Generate until max length or pen state is \"end\"\n    while inputs.shape[1] < 100 and inputs[0, -1, -1] != 1:\n        # Create mask for current sequence length (including class token)\n        seq_len = inputs.shape[1]\n        mask = create_mask(1, seq_len)\n        \n        # Forward pass with label\n        offsets, pen_states = model(inputs, label, mask)\n        offsets = np.round(offsets)\n        pen_states = np.round(pen_states)\n        # Get the last predicted stroke (offset + pen state)\n        pred = np.concatenate((offsets[0, -1], pen_states[0, -1]))\n        inputs = np.concatenate((inputs, pred.reshape(1, 1, 5)), axis=1)\n    \n    return inputs[0]","metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":"def create_mask(batch_size, seq_len):\n    \"\"\"\n    Creates look-ahead mask for sequences including the class token.\n    \"\"\"\n    mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n    # mask = mask[tf.newaxis, tf.newaxis, :, :]  # Shape: [1, 1, seq_len, seq_len]\n    return mask","metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":"filepaths","metadata":{},"outputs":[{"data":{"text/plain":["['DATASET/sketchrnn_pillow.npz',\n"," 'DATASET/sketchrnn_diamond.npz',\n"," 'DATASET/sketchrnn_bench.npz',\n"," 'DATASET/sketchrnn_knife.npz',\n"," 'DATASET/sketchrnn_screwdriver.npz',\n"," 'DATASET/sketchrnn_finger.npz',\n"," 'DATASET/sketchrnn_lightning.npz',\n"," 'DATASET/sketchrnn_circle.npz',\n"," 'DATASET/sketchrnn_candle.npz',\n"," 'DATASET/sketchrnn_suitcase.npz']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"execution_count":11},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\ndef generate_sketch(model, label, max_len, max_steps=250):\n    num_classes = model.num_classes\n    label_one_hot = np.zeros((1, num_classes), dtype=np.int32)\n    label_one_hot[0, label] = 1.0\n    label_tensor = tf.constant(label_one_hot, dtype=tf.float32) \n    strokes = []  \n\n    for _ in range(max_steps):\n        \n        if len(strokes) == 0:\n            \n            stroke_input = tf.zeros((1, 0, 5), dtype=tf.float32)  \n        else:\n            \n            stroke_input = tf.convert_to_tensor(strokes, dtype=tf.float32)  \n            stroke_input = stroke_input[tf.newaxis, :, :]  \n\n        seq_len = len(strokes) + 1  \n        if seq_len > max_len:\n            stroke_input = stroke_input[:, -max_len + 1:, :]  \n            seq_len = max_len\n        mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)  \n        offsets, pen_states = model(stroke_input, label_tensor, mask, training=False)\n        next_offset = offsets[:, -1, :]  \n        next_pen = pen_states[:, -1, :]  \n        pen_idx = tf.argmax(next_pen, axis=-1)  \n        pen_one_hot = tf.one_hot(pen_idx, depth=3)  \n        next_stroke = tf.concat([next_offset, pen_one_hot], axis=-1)  \n        strokes.append(next_stroke[0].numpy()) \n        if tf.reduce_all(tf.equal(pen_one_hot, [0, 0, 1])):\n            break\n\n    return np.array(strokes)","metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":"cat_sketch = generate_sketch(\n    model=small_model,\n    label=9,  # \"cat\"\n    max_len=100,  # Must match training max_len\n    max_steps=250\n)","metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":"print(\"Generated sketch shape:\", cat_sketch.shape)\nprint(\"First few strokes:\", cat_sketch[:5])","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated sketch shape: (16, 5)\n","First few strokes: [[10.924255  34.85005    1.         0.         0.       ]\n"," [13.800518  36.290554   1.         0.         0.       ]\n"," [22.190905  22.47266    1.         0.         0.       ]\n"," [44.70678   12.354386   1.         0.         0.       ]\n"," [62.62969   -6.0382643  1.         0.         0.       ]]\n"]}],"execution_count":33},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef render_sketch(strokes):\n    x, y = 0, 0\n    x_coords = [x]\n    y_coords = [y]\n    for stroke in strokes:\n        dx, dy, p1, p2, p3 = stroke\n        if p1 == 1:  # Pen down\n            x += dx\n            y += dy\n            x_coords.append(x)\n            y_coords.append(y)\n        elif p2 == 1:  # Pen up\n            x += dx\n            y += dy\n            x_coords.append(None)  # Break line\n            y_coords.append(None)\n            x_coords.append(x)\n            y_coords.append(y)\n        elif p3 == 1:  # End\n            break\n    plt.plot(x_coords, y_coords, 'b-')\n    plt.gca().invert_yaxis()\n    plt.title(f\"Generated Sketch \")\n    plt.show()\n\nrender_sketch(cat_sketch)","metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"execution_count":34}]}